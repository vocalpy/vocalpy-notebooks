{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1066df-cb84-4746-a554-e66aa0682020",
   "metadata": {
    "tags": []
   },
   "source": [
    "![VocalPy logo](./images/vocalpy-primary.png)\n",
    "# VocalPy üêç üí¨ in 15 minutes ‚è≤Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01678bef-6ff1-4725-9b65-061b8831c4ef",
   "metadata": {},
   "source": [
    "This tutorial will introduce you to VocalPy, a core Python package for acoustic communication research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f377469-5032-4786-9f24-b79f9d35e3d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c1b958-4732-4602-8c6a-ca2dbdf0263c",
   "metadata": {
    "tags": []
   },
   "source": [
    "First we download some example data, from the [Bengalese Finch song repository](https://nickledave.github.io/bfsongrepo/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae3af9-ae2f-4336-82b5-258524d081b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!curl -sSL https://raw.githubusercontent.com/vocalpy/vak/main/src/scripts/download_autoannotate_data.py | python3 -"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed2ff32-c82a-460d-a50d-d9b7d8ce7338",
   "metadata": {
    "tags": []
   },
   "source": [
    "And then we'll move that data into a `./data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b07a9b1-b3c5-4482-b4d2-ee19b3744745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0b0f4-d3b9-4525-a341-9eae3191a2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pathlib.Path('./data').mkdir(exist_ok=True,parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b564f-37c1-4673-adc9-19596c99afc0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pathlib.Path('./bfsongrepo').rename('./data/bfsongrepo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcab852-e274-409e-bbec-e9f379b27bbd",
   "metadata": {},
   "source": [
    "Now that we've got some data to work with, we can import `vocalpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a0b012-3b48-4ca5-b5a5-ca4c5fed5c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vocalpy as voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087795d-55d1-4c01-9b0e-d159f5422799",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data types for acoustic communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee62d0bb-45b4-48bf-820a-d7d197ade6e7",
   "metadata": {},
   "source": [
    "Let's look at the data types that VocalPy provides for acoustic comunication."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb87c9-a40a-4ba7-b7bc-cb05d1b1662c",
   "metadata": {},
   "source": [
    "We load all the wav files from a directory using a convenience function that VocalPy gives us in its `paths`, `vocalpy.paths.from_dir`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7c9e4-bc79-43e7-8d22-695a90bbb069",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = ('data/bfsongrepo/gy6or6/032312/')\n",
    "\n",
    "wav_paths = voc.paths.from_dir(data_dir, 'wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b362189e-0355-4416-a92e-75431897d5da",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data type for audio: `vocalpy.Audio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf1c7f-c1a6-49a0-a976-a57a88b948e7",
   "metadata": {},
   "source": [
    "Next we load all the wav files into the data type that VocalPy provides for audio, `vocalpy.Audio`, using the method `vocalpy.Audio.read`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3081c4ea-7beb-4424-b449-00fb4d6ce2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audios = [\n",
    "    voc.Audio.read(wav_path) for wav_path in wav_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7ab1d-ffa9-4d28-9e72-519ba9daaa52",
   "metadata": {},
   "source": [
    "Let's inspect one of the `vocalpy.Audio` instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe205ac-7ae8-4586-8b07-12a76c60a81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "an_audio = audios[0]\n",
    "print(an_audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76788123-620e-4d54-81a6-70fd74778a6b",
   "metadata": {},
   "source": [
    "We can see that it has four attributes:\n",
    "\n",
    "1. `data`, the audio signal itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7090a5d-352f-446f-b1d9-178b90acc834",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(an_audio.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5256c41c-47bb-4dfe-99ea-80114a6b2fbe",
   "metadata": {},
   "source": [
    "2. `samplerate`, the sampling rate for the audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606a72ca-df5a-4305-a7a6-6b26bd27cb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(an_audio.samplerate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47da049-95be-4cc6-b2b0-21b273f31693",
   "metadata": {},
   "source": [
    "3. `channels`, the number of channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f3997-9a7f-466d-bd7d-5fe3706e280c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(an_audio.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdace1b-4061-4327-a116-07ed3d54a94d",
   "metadata": {},
   "source": [
    "and finally,  \n",
    "\n",
    "4. `path`, the path to the file that the audio was read from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a747c5-42dd-431a-887a-5290d7620f67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(an_audio.path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b658a-1832-4267-8ca0-c83acb99bbd1",
   "metadata": {},
   "source": [
    "One of the reasons VocalPy provides this data type, and the others we're about to show you here, is that it helps you write more succinct code that's easier to read: for you, when you come back to your code months from now, and for others that want to read the code you share."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0082e69-84c0-456a-ba02-ff4d9125fa98",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Classes for steps in pipelines for processing data in acoustic communication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b1a02-2486-4477-9592-2d9e5c4c4c17",
   "metadata": {},
   "source": [
    "In addition to data types for acoustic communication, VocalPy provides you with classes that represent steps in pipelines for processing that data. These classes are also written with readability and reproducibility in mind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6100fc6-b08b-419f-89bc-90d158fa993f",
   "metadata": {},
   "source": [
    "Let's use one of those classes, `SpectrogramMaker`, to make a spectrogram from each one of the wav files that we loaded above.\n",
    "\n",
    "We'll write a brief snippet to do so, and then we'll explain what we did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9886a3-6add-4e0f-9bf4-5ccd12d12159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spect_params = {'fft_size': 512, 'step_size': 64}\n",
    "callback = voc.signal.spectrogram.spectrogram\n",
    "spect_maker = voc.SpectrogramMaker(callback=callback, spect_params=spect_params)\n",
    "spects = spect_maker.make(audios, parallelize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa817393-e41d-40df-8cfb-76a12d3aebc9",
   "metadata": {},
   "source": [
    "Notice a couple of things about this snippet:\n",
    "- In line 1, you declare the parameters that you use to generate spectrograms explicitly, as a dictionary. This helps with reproducibility by encouraging you to document those parameters\n",
    "- In line 2, you also decide what code you will use to generate the spectrograms, by using what's called a \"callback\", because the `SpectrogramMaker` will call this function for you.\n",
    "- In line 3, you create an instance of the `SpectrogramMaker` class with the function you want to use to generate spectrograms, and the parameters to use with that function.\n",
    "- In line 4, you make the spectrograms, with a single call to the method `vocalpy.SpectrogramMaker.make`. You pass in the audio we loaded earlier, and you tell VocalPy that you want to parallelize the generation of the spectrograms. This is done for you, using the library `dask`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232fec0e-baf5-484f-9be7-8192ad3a5dbc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Data type: `vocalpy.Spectrogram`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69e924a-6ef3-42bd-9eaf-b5cefae75743",
   "metadata": {},
   "source": [
    "As you might have guessed, when we call `SpectrogramMaker.make`, we get back a list of spectrograms.\n",
    "\n",
    "This is the next data type we'll look at. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc116c95-6805-47ec-9843-5bd1401ed6fc",
   "metadata": {},
   "source": [
    "We inspect the first spectrogram we loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9259b205-4a6b-4d33-9760-20eb5a2aacc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_spect = spects[0]\n",
    "print(a_spect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad750b08-2020-480e-a75a-9a2a6a3283fc",
   "metadata": {},
   "source": [
    "As before, we'll walk through the attributes of this class.\n",
    "But since the whole point of a spectrogram is to let us see sound, let's actually look at the spectrogram, instead of staring at arrays of numbers.\n",
    "\n",
    "We'll make a new spectrogram where we log transform the data so it's easier to visualize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8029e6-fb6f-4bb1-b210-8d21ea29b0b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "We import NumPy so we can do a quick-and-dirty transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f9355-a2d7-4728-b498-a0f7961ef5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc6438-5ef4-4fd9-8a13-04c3d38b5b42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a_spect_log = voc.Spectrogram(data=np.log(a_spect.data),\n",
    "                              frequencies=a_spect.frequencies,\n",
    "                              times=a_spect.times)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790f7be4-a578-461e-b0dd-3b6338ed9eee",
   "metadata": {},
   "source": [
    "Then we'll plot the log-transformed spectrogram using a function built into the `vocalpy.plot` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a757c-7e25-4088-8627-f854db71c4b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc.plot.spectrogram(\n",
    "    a_spect_log,\n",
    "    tlim = [2.6, 4],\n",
    "    flim=[500,12500],\n",
    "    pcolormesh_kwargs={'vmin':-25, 'vmax': -10}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7ed706-7998-4c16-aec8-46d2469d87a2",
   "metadata": {},
   "source": [
    "We see that we have a spectrogram of Bengalese finch song.\n",
    "\n",
    "Now that we know what we're working with, let's actually inspect the attributes of the `vocalpy.Spectrogram` instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680a4d65-a33a-4173-a8ae-d3785098c66b",
   "metadata": {},
   "source": [
    "There are five attributes we care about here.\n",
    "\n",
    "1. `data`: this is the spectrogram itself -- as with the other data types,like `vocalpy.Audio`, the attribute name `data` indiciates this main data we care about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8401b866-1850-4499-9ffb-91b2e217f508",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ba26aa-21be-417c-93e1-6037c6d3d8f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's look at the shape of `data`. It's really just a NumPy array, so we inspect the array's `shape` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e62df5d-55a1-43b1-981f-2191ecad7623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e9a79c-20d5-44f3-a76e-abf081e407b9",
   "metadata": {},
   "source": [
    "We see that we have a matrix with some number of rows and columns. These correspond to the next two attributes we will look at."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05bfc1f-1567-4b60-afce-4a5be6488d7e",
   "metadata": {},
   "source": [
    "2. `frequencies`, a vector of the number of frequency bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86edcd68-09d8-4529-b77c-86222fde04ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.frequencies[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b82fb6f-3723-4b77-9e87-84a52b23d6f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.frequencies.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aca061-28fb-47da-ba56-212578a9f0eb",
   "metadata": {},
   "source": [
    "(We see it is equal to the number of rows.)\n",
    "\n",
    "3. `times`, a vector of time bin centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494f90a5-824f-4b5e-ab77-048568899801",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.times[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2587378-6d63-48c9-9530-58b0b60fda0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(a_spect.times.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c2e972-dd16-4da2-8bdc-7565160930f6",
   "metadata": {},
   "source": [
    "Just like with the `Audio` class, VocalPy gives us the ability to conveniently read and write spectrograms from files. This saves us from generating spectrograms over and over. Computing spectrograms can be computionally expensive, if your audio has a high sampling rate or you are using methods like multi-taper spectrograms. Saving spectrograms from files also makes it easier for you to share your data in the exact form you used it, so that it's easier to replicate your analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9830851-b20e-4dcb-bd00-83037f46fb50",
   "metadata": {},
   "source": [
    "To see this in action, let's write our spectrograms to files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f45a73-15f2-49cf-9d27-b45beed87840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "for spect in spects:\n",
    "    spect.write(\n",
    "        spect.audio_path.parent / (spect.audio_path.name + '.spect.npz')\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c01412-5c9f-4b6a-b478-3e11e7a594d3",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notice that the extension is `'npz'`; this is a file format that NumPy uses to save mulitple arrays in a single file. By convention we include the file extension of the source audio, and another \"extension\" that incidicates this is a spectrogram, so that the file name ends with `'.wav.spect.npz'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716ae37-5f27-432a-82ac-b25944a20d59",
   "metadata": {},
   "source": [
    "We can confirm that reading and writing spectrograms to disk works as we expect using the method `vocalpy.Spectrogram.read`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d89101-ca40-4d7e-a177-7eab1089f82f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spect_paths = voc.paths.from_dir(data_dir, '.wav.spect.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9434bad-0b1d-45f3-861b-902c40e42dbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spects_loaded = [\n",
    "    voc.Spectrogram.read(spect_path)\n",
    "    for spect_path in spect_paths\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6c5ca-1b9c-400b-a87b-ff475908408b",
   "metadata": {},
   "source": [
    "We compare with the equality operator to confirm we loaded what we saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c7bdf8-0332-442f-97c3-d5e15f3cb124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this happens to work \n",
    "# because VocalPy always gives us back `sorted` lists,\n",
    "# but it wouldn't work in the more general case--\n",
    "# we'd need to pair by filename first or something\n",
    "for spect, spect_loaded in zip(spects, spects_loaded):\n",
    "    assert spect == spect_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc392487-b7ab-4df8-ac4b-fb6c33ddea7b",
   "metadata": {},
   "source": [
    "### Data type: `vocalpy.Annotation`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fffeb0-25a5-41ff-9bb5-fe06a9c7d847",
   "metadata": {},
   "source": [
    "The last data type we'll look at is for annotations. Such annotations are important for analysis of aocustic communication and behavior. Under the hood, VocalPy uses the pyOpenSci package [crowsetta](https://github.com/vocalpy/crowsetta)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b168e0-2cee-4c52-9b62-176696862353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import vocalpy as voc\n",
    "\n",
    "csv_paths = voc.paths.from_dir(data_dir, '.wav.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8d33e-bc87-4b61-bfc9-3d468b9eeeab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annots = [voc.Annotation.read(notmat_path, format='simple-seq') \n",
    "          for notmat_path in csv_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92fd091-b441-40a9-a5a5-204dd63860a9",
   "metadata": {},
   "source": [
    "We inspect one of the annotations. Again as with other data types, we can see there is a `data` attribute. In this case it contains the `crowsetta.Annotation`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b527ccc6-bf87-42b1-8736-1de4d18b3c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(annots[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82ab319-042d-4f78-a29d-3af09951ef25",
   "metadata": {},
   "source": [
    "We plot the spectrogram along with the annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb918367-6f66-43d5-96a7-c4a9652771d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "voc.plot.annotated_spectrogram(\n",
    "    spect=voc.Spectrogram(data=np.log(spects[1].data),\n",
    "                    frequencies=spects[1].frequencies,\n",
    "                    times=spects[1].times),\n",
    "    annot=annots[1],\n",
    "    tlim = [3.2, 3.9],\n",
    "    flim=[500,12500],\n",
    "    pcolormesh_kwargs={'vmin':-25, 'vmax': -10}\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721cff72-1b83-4f32-9f00-b7156b25b98b",
   "metadata": {},
   "source": [
    "This crash course in VocalPy has introduced you to the key features and goals of the library. To learn more, please check out [the documentation](https://vocalpy.readthedocs.io/en/latest/) and read our Forum Acusticum 2023 Proceedings Paper, [\"Introducing VocalPy\"](https://github.com/vocalpy/vocalpy/blob/main/docs/fa2023/Introducing_VocalPy__a_core_Python_package_for_researchers_studying_animal_acoustic_communication.pdf). We are actively developing the library to meet your needs and would love to hear your feedback in [our forum](https://forum.vocalpy.org/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

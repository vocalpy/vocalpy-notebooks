{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3554a002-ca08-487e-81de-4f0ffc0773e5",
   "metadata": {},
   "source": [
    "# Embedding Bengalese finch song syllables in latent space with UMAP and VocalPy\n",
    "\n",
    "This is a demo of embedding the syllables of Bengalese finch song in a two-dimensional space with the dimensionality reduction algorithm [UMAP (Uniform Manifold Approximation & Projection)](https://umap-learn.readthedocs.io/en/latest/how_umap_works.html).\n",
    "\n",
    "For a more general guide please see:  \n",
    "https://besjournals.onlinelibrary.wiley.com/doi/full/10.1111/1365-2656.13754\n",
    "and the accompanying GitHub repository https://github.com/marathomas/tutorial_repo,\n",
    "as well as the original paper and repository from Sainburg et al. 2020:\n",
    "* https://github.com/timsainb/avgn_paper/tree/V2\n",
    "* https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008228"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c362e27-98e7-4025-9ca3-2f14745346aa",
   "metadata": {},
   "source": [
    "Import VocalPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26db875-a8e0-4c86-9bf8-e9772f40762a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import vocalpy as voc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a4269-9198-451c-9222-58801f2b8316",
   "metadata": {},
   "source": [
    "Load data (that we downloaded in notebook 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5d28f5-1e22-4aec-ad7c-776177739534",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_dir = ('data/bfsongrepo/gy6or6/032312/')\n",
    "\n",
    "wav_paths = voc.paths.from_dir(data_dir, 'wav')\n",
    "csv_paths = voc.paths.from_dir(data_dir, '.wav.csv')  # annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a2ec19-de7c-4eee-ab29-f37ca72d0ae8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import crowsetta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630f7d94-33bc-4b43-bdab-e839dc19a13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "audios = [voc.Audio.read(wav_path) for wav_path in wav_paths]\n",
    "annots = [voc.Annotation.read(csv_path, format='simple-seq') for csv_path in csv_paths]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3642d222-5f2b-41cd-8bbd-eb3a3ec157de",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c57fec-c19c-44d0-ad5e-a05c386c5210",
   "metadata": {},
   "source": [
    "In this notebook we want to embed the syllables of Bengalese finch song with UMAP.  \n",
    "First we prepare the dataset.\n",
    "\n",
    "The first step requires us to have segmented the audio into constituent [units of analysis](https://onlinelibrary.wiley.com/doi/abs/10.1111/brv.12160?casa_token=OzzQcd7O9AQAAAAA:jkcnLBuWBlia0tHFmffTMziy9cXtAVwVraJm43mw7GQgDqGsmpZ-9omjv4X6FABVnd7KMcZST1Gl8mA), in this case the syllables of Bengalese finch song. It is also helpful to have annotated the existing annotations with some number of classes, especially for the purpose of inspecting the results of the UMAP embedding.\n",
    "\n",
    "There are of course many methods to segment and annotate animal sounds, that are beyond the scope of this tutorial (we might be slightly biased but we recommend you try [TweetyNet](https://github.com/yardencsGitHub/tweetynet), that is now built into the [vak](https://github.com/vocalpy/vak) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af62ef-2d5b-4c11-aa3a-71f1742b5787",
   "metadata": {},
   "source": [
    "We use a helper function to load each audio file and then use the corresponding annotation to get the audio for each annotated segment--a birdsong syllable--as well as the label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b0707c-5e79-4339-9850-74a4ba05ed63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_segment_audio(audios, annots, labelset=list('iabcdefghjk'), min_length=1024):\n",
    "    \"\"\"Get audio for every segment in every sequence.\n",
    "    Pad audio for each segment so they are all the same size.\n",
    "    Here we use the naive approach of just padding all segments \n",
    "    with neighoring audio so they are the same size as the largest.\n",
    "    Note this might not be a good idea for the general case,\n",
    "    if there are very brief silent gaps\n",
    "    (or no silent gaps) between segments.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    audios : list\n",
    "        of vocalpy.Audio\n",
    "    annots: list\n",
    "        of vocalpy.Annotation, annotated sequences in ``audios``\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    audios_out : list\n",
    "        vocalpy.Audio instances that contain just the segment clips\n",
    "    labels_out : list\n",
    "        The label given to each segment in the annotations\n",
    "    \"\"\"\n",
    "    audios_out = []\n",
    "    labels_out = []\n",
    "    for annot, audio in zip(annots, audios):\n",
    "\n",
    "        # ---- convert sequence of units to array of starts and stop times\n",
    "        start_times = annot.data.seq.onsets_s\n",
    "        stop_times = annot.data.seq.offsets_s\n",
    "\n",
    "        start_inds = np.round(start_times * audio.samplerate).astype(int)\n",
    "        stop_inds = np.round(stop_times * audio.samplerate).astype(int)\n",
    "        \n",
    "        labels = annot.data.seq.labels\n",
    "\n",
    "        for start, stop, label in zip(start_inds, stop_inds, labels):\n",
    "            if label not in labelset:\n",
    "                continue\n",
    "            segment_audio = audio.data[start:stop]\n",
    "            if segment_audio.shape[-1] < min_length:\n",
    "                continue\n",
    "            audios_out.append(\n",
    "                voc.Audio(data=segment_audio, samplerate=audio.samplerate)\n",
    "            )\n",
    "            labels_out.append(label)\n",
    "    return audios_out, labels_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74819efb-c4ae-414b-b7f1-2b0f9d88f17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "segment_audios, segment_labels = get_segment_audio(audios, annots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82efe33-efc8-488f-9717-f8279492d850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "label_audio_map = defaultdict(list)\n",
    "\n",
    "MAX_NUM_AUDIOS = 700\n",
    "\n",
    "for audio, label in zip(segment_audios, segment_labels):\n",
    "    if len(label_audio_map[label]) < MAX_NUM_AUDIOS:\n",
    "        label_audio_map[label].append(audio)\n",
    "\n",
    "# we throw away the old lists and make new ones that we use below\n",
    "segment_labels, segment_audios = [], []\n",
    "for label in label_audio_map.keys():\n",
    "    segment_audios.extend(label_audio_map[label])\n",
    "    segment_labels.extend([label] * len(label_audio_map[label]))\n",
    "del label_audio_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3978d-5358-4e18-96a6-a35d3caa1317",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spect_params = {'fft_size': 512, 'step_size': 64, 'transform_type': 'log_spect'}\n",
    "callback = voc.signal.spectrogram.spectrogram\n",
    "spect_maker = voc.SpectrogramMaker(callback=callback, spect_params=spect_params)\n",
    "segment_spects = spect_maker.make(segment_audios, parallelize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545a881b-42c6-4122-86af-58701b364ba6",
   "metadata": {},
   "source": [
    "As in Sainburg et al. 2020 and Thomas et al. 2022, we pad all spectrograms so they are the same \"width\" (number of columns) as the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4b8ca6-e775-4cca-b681-231c15508d47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "widths = [\n",
    "    segment_spect.data.shape[-1]\n",
    "    for segment_spect in segment_spects\n",
    "]\n",
    "pad_length = max(widths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e64408b-acac-4867-9be5-15ecb803b31b",
   "metadata": {},
   "source": [
    "We also min-max scale the spectrogram so the values lie between 0 and 1, again as in Sainburg et al. 2020 and Thomas et al. 2022. This avoids a situation where e.g. a spectrogram in decibels has negative values, and we pad with 0 which is larger than the negative value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3340fc-3e03-40c9-9cc4-6d49a063db1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def minmax_scale(spectrogram):\n",
    "    \"\"\"Min-max scales the spectrogram data to be between 0 and 1\"\"\"\n",
    "    data = spectrogram.data\n",
    "    min_ = data.min()\n",
    "    max_ = data.max()\n",
    "    return (data - min_) / (max_ - min_)\n",
    "\n",
    "\n",
    "def pad_spectrogram(data, pad_length):\n",
    "    \"\"\" Pads spectrogram array to being a certain width \n",
    "    (columns, assumed to be time bins)\"\"\"\n",
    "    excess_needed = pad_length - np.shape(data)[1]\n",
    "    pad_left = np.floor(float(excess_needed) / 2).astype(\"int\")\n",
    "    pad_right = np.ceil(float(excess_needed) / 2).astype(\"int\")\n",
    "    return np.pad(\n",
    "        data, [(0, 0), (pad_left, pad_right)], \"constant\", constant_values=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5201d83f-7ae5-48dc-97b1-d7603c8862e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this is a nested list comprehension, not the easiest thing to read\n",
    "X = np.array(\n",
    "    [pad_spectrogram(\n",
    "        # we minmax scale *first*, then pad the returned array\n",
    "        minmax_scale(segment_spect), pad_length\n",
    "    ) for segment_spect in segment_spects]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a63f2a5-86ae-4bb4-80cd-4e84c64aab73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1f6346-01a3-41e4-97d4-a40771f1e0cc",
   "metadata": {},
   "source": [
    "We visualize the data after the transformations, to inspect what's going in to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1e010f-9cb0-4c73-bae4-604a2db7e09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf4ac28-277b-43fb-a74d-3b7cbcda30e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax_arr = plt.subplots(5, 5, dpi=300)\n",
    "ax_arr = ax_arr.ravel()\n",
    "\n",
    "inds = np.random.permutation(np.arange(X.shape[0]))[:25]\n",
    "\n",
    "to_viz = X[inds]\n",
    "\n",
    "for x, ax in zip(to_viz, ax_arr):\n",
    "    ax.imshow(x)\n",
    "    ax.set_axis_off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03934948-fdc7-463b-a64f-5b9fda7f38f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = X.reshape(X.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe5bf98-00cb-4ba6-ba0a-02893fcc28b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3506e3c-dfb9-4b12-b778-e72338f27172",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Embed with UMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ffd200-373c-44cf-bcb1-7426a36fe12d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddd1f1-7cee-41b4-addd-bf31a5658b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "umapper = umap.UMAP(min_dist=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f6a42-e68b-4325-994a-83e9eca88593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out = umapper.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e68648e-b1cf-4998-aaed-1469ef5a7c2a",
   "metadata": {},
   "source": [
    "We confirm that we have embedded the syllables in a two-dimensonal space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bfd052-0473-4a1f-a071-5242ad24ec07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac70a9b-6415-4eb8-b54a-4af1b986044a",
   "metadata": {},
   "source": [
    "## Visualize the embeddings\n",
    "adapted from https://github.com/timsainb/avgn_paper/blob/V2/avgn/visualization/projections.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2de4c9b-38a2-4e38-afc5-89d3c5ed18ff",
   "metadata": {},
   "source": [
    "We make a color palette for the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702b873-3fcd-46cd-b53b-cd84366211d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.set('notebook')\n",
    "segment_labels = np.array(segment_labels)\n",
    "pal = sns.color_palette(\"tab20\", n_colors=len(np.unique(segment_labels)))\n",
    "label_color_map = {lab: pal[i] for i, lab in enumerate(np.unique(segment_labels))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11c842a-5a9a-4589-b4b3-357b6169dd87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for label in np.unique(segment_labels):\n",
    "    inds = segment_labels == label\n",
    "    ax.scatter(\n",
    "        out[inds, 0],\n",
    "        out[inds, 1],\n",
    "        alpha=0.25,\n",
    "        s=5,\n",
    "        color=label_color_map[label],\n",
    "        label=label,\n",
    "    )\n",
    "\n",
    "ax.legend(loc='center left', bbox_to_anchor=[1, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d597bc14-1796-41bd-8c3e-f2fe4db62f3e",
   "metadata": {},
   "source": [
    "This is a simple demo where we have not fine tuned the data preparation or the embedding step. An alternative approach is to replace one step of UMAP with a neural network model as described in [Sainburg et al. 2021](https://direct.mit.edu/neco/article/33/11/2881/107068), known as Parametric UMAP (because the neural network \"parametrizes\" the transform that embeds the high-dimensional segments).\n",
    "\n",
    "We have not yet carefully benchmarked various approaches but our initial tests with Parametric UMAP, as implemented in the vak library, suggest that it can yield more consistent clusters for this specific dataset. See results here: https://conference.scipy.org/proceedings/scipy2023/pdfs/david_nicholson.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
